# -*- coding: utf-8 -*-
"""Web_Scrapping Practice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VecV2j-VjgXr8t7DTKTgz8stSGC1zpV9
"""

#Extracting Data from https://www.ambitionbox.com/list-of-companies?page=1
#total pages: 333 pages of all the companies in india

# practice of web scrapping - using youtube "https://www.youtube.com/watch?v=RmPVICEWF4w&list=PLKnIA16_RmvZAqJzKstVHywcRNMn6pcGD&index=3"
import requests
from bs4 import BeautifulSoup
import pandas as pd

# you get an error because not allowed to scrap website
requests.get('https://www.ambitionbox.com/list-of-companies?page=1').text

# disgusing the search as a browser search to pass thru the website scrapping denial
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Safari/537.36'}
webpage = requests.get('https://www.ambitionbox.com/list-of-companies?page=1', headers=headers).text

#lxml - allows parsing thru the html web page
soup = BeautifulSoup(webpage, 'lxml')

print(soup.prettify()) # to view the web page in a json format - to go thru the page

soup.find_all('h1') # to find all h1 tag from the html web page on the page - wll return a string
soup.find_all('h1')[0].text # to actually get the stringof the h1 tag

soup.find_all('h2') # find all the h2 tags - will return a string

#lenght of all the content with the h2 tag
len(soup.find_all('h2'))

# to find the list of company name with h2 tag in the html web page -  will return just the names
for i in soup.find_all('h2'):
  print(i.text.strip()) # .strip will remove the html format of spacing done using "/n"

# we need rating of the companies also, it is in p tag
soup.find_all('p', class_='rating')

# to find all reviews
soup.find_all('a', class_='review-count')

#NOTE: where do I find tags and the class name - from the web page's inspect, click on the component and find these details

# to find if the company is public
soup.find_all('p', class_='infoEntity') # but this will fetch thing like how old it is, location and number of employees

#just to get one of the info from the 'infoEntity' - the class name from web page
# fetch the entire block of the company's tag - it might be a block (wrapper - html wrapper)

# **MAIN CODE** - just fetching one page then you can do for all the pages also
import numpy as np

company = soup.find_all('div', class_='company-content-wrapper') #o/p will be a list (find_all ka output will be list)
#len(company) = 30 [because 30 companies in a page]

#find_all - to get multiple finds/answers
#find - to get just one
name = []
for i in company:
  name.append(i.find('h2')).text.strip() # to find the names of all the companies from each div
  name

rating =[]
for i in company:
  rating.append(i.find_all('p'), class_='rating')[0].text.strip() #[0] - because 'p' tag has multiple items (if using find_all or use find and not use [0])


reviews =[]
for i in company:
  reviews.append(i.find('a', class_='review-count')[0].text.strip())

c_type =[]
hq =[]
old =[]
employees =[]
about = []

for i in company:
    c_type.append(i.find_all('p', class_='infoEntity')[0].text.strip())
    hq.append(i.find_all('p', class_='infoEntity')[1].text.strip())
    old.append(i.find_all('p', class_='infoEntity')[2].text.strip())
    try:
        employees.append(i.find_all('p', class_='infoEntity')[3].text.strip())
    except:
        employees.append(np.nan)

     # OR:

    #if len(i.find_all('p', class_='infoEntity')) < 4:
     #   employees.append('nan')
    #else:
    # employees.append(i.find_all('p', class_='infoEntity')[3].text.strip())
    about.append(i.find('p').text.strip())

df = pd.DataFrame({'name':name, 'rating':rating, 'reviews':reviews, 'company type':c_type, 'headquater':hq, 'age of the company':old, 'no. of employees':employees, 'about':about})
df.shape
df.head(5)

#for say 11 pages
final = pd.DataFrame()
for j in range(1,11):
  url = 'https://www.ambitionbox.com/list-of-companies?page={}'.format(j)
  headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Safari/537.36'}

  webpage = requests.get(url, headers=headers).text
  soup = BeautifulSoup(webpage, 'lxml')
  company = soup.find_all('div', class_='company-content-wrapper')

  #then the entire loop of finding data comes in here